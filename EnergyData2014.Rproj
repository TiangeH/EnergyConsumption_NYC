Version: 1.0

RestoreWorkspace: Default
SaveWorkspace: Default
AlwaysSaveHistory: Default

EnableCodeIndexing: Yes
UseSpacesForTab: Yes
NumSpacesForTab: 2
Encoding: UTF-8

RnwWeave: Sweave
LaTeX: pdfLaTeX

#1.import data
nyc_data<-read.csv(file = "C:/Users/Tiange/Desktop/Capstone_Project/nyc_2014_raw.csv",header = T,sep=',')

nrow(nyc_data)
#install packages and rename the data since many attributes' name is not convenient for us to use
install.packages("gtools")#for quantcut function
library(gtools) #for quantcut
library(sqldf)
library(ggplot2)
library(randomForest)
library(data.table)
library(xgboost)
library(xlsx) #export to excel

#2 Data exploration and restructure
str(nyc_data)
nrow(nyc_data)

#change column name etc
colnames(nyc_data)<-c("RecordNumber",
                      "BBL",
                      "Co_BBLStatus",
                      "BBLsCo_reported",
                      "BIN",
                      "StreetNumber",
                      "StreetName",
                      "Borough",
                      "ZipCode",
                      "BBLontheCoveredBuildingsList",
                      "DOFBenchmarkingSubmissionStatus",
                      "SiteEUI_kBtu_ft2",
                      "WeatherNormalizedSiteEUI_kBtu_ft2","SourceEUIkBtu_ft2",
                      "WeatherNormalizedSourceEUIkBtu_ft2",
                      "MunicipallySuppliedPotableWater_IndoorIntensity_gal_ft",
                      "AutomaticWaterBenchmarkingEligible",
                      "ReportedWaterMethod",
                      "ENERGYSTARScore",
                      "TotalGHGEmissions_MtCO2e",
                      "DirectGHGEmissions_MtCO2e",
                      "IndirectGHGEmissions_MtCO2e",
                      "ReportedPropertyFloorArea_Buildings_ft",
                      "DOFPropertyFloorArea_BuildngsandParking_ft2",
                      "PrimaryPropertyType",
                      "DOFNumberofBuildings")

#remove those with In Violation because those rwos contains most missing value
nyc_data<-sqldf("select * from nyc_data 
                where DOFBenchmarkingSubmissionStatus != 'In Violation ' 
                and DOFBenchmarkingSubmissionStatus!= 'Not Applicable' 
                and ENERGYSTARScore != 'Not Available' 
                and ENERGYSTARScore!= '0'
                and ENERGYSTARScore!='See Primary BBL'
                ")



#convert factor to numerica, otherwise we can't do further analysis

nyc_data$RecordNumber<-as.numeric(nyc_data$RecordNumber)
nyc_data$SiteEUI_kBtu_ft2<-as.numeric(as.character(nyc_data$SiteEUI_kBtu_ft2))
nyc_data$WeatherNormalizedSiteEUI_kBtu_ft2<-as.numeric(as.character(nyc_data$WeatherNormalizedSiteEUI_kBtu_ft2))
nyc_data$SourceEUIkBtu_ft2<-as.numeric(as.character(nyc_data$SourceEUIkBtu_ft2))
nyc_data$WeatherNormalizedSourceEUIkBtu_ft2<-as.numeric(as.character(nyc_data$WeatherNormalizedSourceEUIkBtu_ft2))
nyc_data$MunicipallySuppliedPotableWater_IndoorIntensity_gal_ft<-as.numeric(as.character(nyc_data$MunicipallySuppliedPotableWater_IndoorIntensity_gal_ft))
nyc_data$ENERGYSTARScore<-as.numeric(as.character(nyc_data$ENERGYSTARScore))
nyc_data$TotalGHGEmissions_MtCO2e<-as.numeric(as.character(nyc_data$TotalGHGEmissions_MtCO2e))
nyc_data$DirectGHGEmissions_MtCO2e<-as.numeric(as.character(nyc_data$DirectGHGEmissions_MtCO2e))
nyc_data$IndirectGHGEmissions_MtCO2e<-as.numeric(as.character(nyc_data$IndirectGHGEmissions_MtCO2e))
nyc_data$ReportedPropertyFloorArea_Buildings_ft<-as.numeric(as.character(nyc_data$ReportedPropertyFloorArea_Buildings_ft))
nyc_data$DOFPropertyFloorArea_BuildngsandParking_ft2<-as.numeric(as.character(nyc_data$DOFPropertyFloorArea_BuildngsandParking_ft2))

#make sure the change don't change numbers
summary(nyc_data$ENERGYSTARScore)
levels(nyc_data$ENERGYSTARScore)


#3 feature selections

#check the Borough
levels(nyc_data$Borough)
tables(nyc_data$Borough)

nyc_data$Borough[nyc_data$Borough == "BRONX               "]<-"BRONX"
nyc_data$Borough[nyc_data$Borough == "BROOKLYN            "]<-"BROOKLYN"
nyc_data$Borough[nyc_data$Borough == "QUEENS              "]<-"QUEENS"
nyc_data$Borough[nyc_data$Borough == "STATEN ISLAND       "]<-"STATEN ISLAND"

#facot Borough attributes, otherwise, will still be 9 levels instead of 5 levels
nyc_data$Borough <- factor(nyc_data$Borough)

#remove those outliers
nyc_data<-sqldf("select * from nyc_data 
          where ENERGYSTARScore >1 and ENERGYSTARScore <100 ")


#as we can see, a lot attributes are highly correlated with each other. For example
#siteEUi and WeatherNormalizedSiteEui.Based on the defiinition on BenchLaw84
#weatherNormalizeSiteEui represent the mean of siteEUi when weather change.
#however, it may not has huge effect for large size building. 
#since then, we have two methods to deal with these.
#As we checked, we have a lot missing WeatherNormalizedSite and WeatherNormalizedSourceEUI
#1 method is remove WeaNormSour and WeaNorSit EUi form our frame
#2 is replace missing vlaue in WeaNormSour and WeaNorSit by site and sourceEUI.

cor.test(nyc_data$SiteEUI_kBtu_ft2,nyc_data$WeatherNormalizedSiteEUI_kBtu_ft2)
#there is 99.99% correlation between SiteEUI and WeatherNormalizeSite EUI
#if we use the summary or quantcut function, we can find that 1/3 of data
#has site EUI larger than 107, we will seperate them and compare correlationship
#between SiteEUI and WeatherNormalizedSiteEUI

nn<-sqldf("select 
          ENERGYSTARScore,
          SourceEUIkBtu_ft2,
          TotalGHGEmissions_MtCO2e,
          ReportedPropertyFloorArea_Buildings_ft, 
          PrimaryPropertyType,
          Borough
          from nyc_data") 
nn<-nn[-c(4864,5730),] #these two rows contain missing value
nn_data<-nn  
nn_data$PrimaryPropertyType <- factor(nn_data$PrimaryPropertyType)
-----------------------------------------------------------------

property_type<-sqldf("select count (*),PrimaryPropertyType from nn group by PrimaryPropertyType order by count(*) desc")



nn_data_PropertyUseType<-model.matrix(~PrimaryPropertyType-1,nn_data)
nn_data_Borough<-model.matrix(~Borough-1,nn_data)

#right now, categorial data are matrix, need to convert them to data frame
nn_data_PropertyUseType<-data.frame(nn_data_PropertyUseType)
nn_data_Borough<-data.frame(nn_data_Borough)

#then combine both categorial and numerical to one data frame nn_knn_c
nn_data<-cbind(nn_data[,c(1:4)],nn_data_PropertyUseType,nn_data_Borough)
str(nn_data)

normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x))) }


nn_data[,1:4] <- as.data.frame(lapply(nn_data[1:4], normalize))





--------------------------------------------------------------------------------------------
#RANDOM FOREST

dtrf = sort(sample(nrow(nn_data), nrow(nn_data)*.7))
rf_train<-nn_data[dtrf,]
rf_validation<-nn_data[-dtrf,]

rf_train_labels <- nn_data[dtrf, 1]
rf_test_labels <- nn_data[-dtrf, 1]

#baseline for all model
#https://www.r-bloggers.com/part-4a-modelling-predicting-the-amount-of-rain/
best_guess<-mean(rf_train$ENERGYSTARScore)
#use the trainging, there will be a 
RMSE_Baseline<-sqrt(mean((best_guess-rf_validation$ENERGYSTARScore)^2))
#o.28 #0.2267699ï¼Œ27.70958
MAE_baseline <- mean(abs(best_guess-rf_validation$ENERGYSTARScore))
#0.24 #0.1895727  23.69592
set.seed(123)

#Random Forest Model
mtry <- sqrt(ncol(nn_data))

output_forest <- randomForest(ENERGYSTARScore ~.,
                              data = rf_train,ntree=120, mtry=13,
                              importance=TRUE)

output_forest

importance(output_forest)
varImpPlot(output_forest)
plot(output_forest) 
which.min(output_forest$mse)
sqrt(output_forest$mse[which.min(output_forest$mse)])


#we should use the predicted value and actual value on testset instead of training set
nn_data_test_residuals<-test_pred_forest-rf_validation$ENERGYSTARScore

hist(nn_data_test_residuals)
#this step is learned from http://uc-r.github.io/random_forests
#we do cross validation on our training data

valid_split <- initial_split(rf_train, .7)

ames_train_v2 <- analysis(valid_split)

ames_valid <- assessment(valid_split)

x_test <- ames_valid[setdiff(names(ames_valid), "ENERGYSTARScore")]

y_test <- ames_valid$ENERGYSTARScore

rf_oob_comp <- randomForest(formula = ENERGYSTARScore ~ .,data= ames_train_v2,
                            xtest   = x_test,ytest   = y_test)

oob <- sqrt(rf_oob_comp$mse)
validation <- sqrt(rf_oob_comp$test$mse)

tibble::tibble(`Out of Bag Error` = oob,`Test error` = validation,ntrees = 1:rf_oob_comp$ntree
) %>%gather(Metric, RMSE, -ntrees) %>%ggplot(aes(ntrees, RMSE, color = Metric)) +geom_line() +
xlab("Number of trees")

#then tune
features <- setdiff(names(rf_train), "ENERGYSTARScore")
m2 <- tuneRF(
  x          = rf_train[features],
  y          = rf_train$ENERGYSTARScore,
  ntreeTry   =  ,
  mtryStart  = 6,
  stepFactor = 1.5,
  improve    = 0.01,
  trace      = FALSE)

# m2
#mtry   OOBError
#4     4 0.02967727
#6     6 0.02337338
#9     9 0.02013521
#13   13 0.01969575
#19   19 0.01996363


#check the performance of the model
test_pred_forest <- predict(output_forest,rf_validation)
RMSE_RF<-sqrt(mean((test_pred_forest-rf_validation$ENERGYSTARScore)^2))
#mtry=2 0.1507 mtry=6 0.1522 after to mtry=13 0.139
MAE_RF<-mean(abs(test_pred_forest-rf_validation$ENERGYSTARScore))
#mtry=2 0.182 mtry=6 0.1207 after mtry=13 0. 102

#to visualize
plot(test_pred_forest,rf_validation$ENERGYSTARScore,xlab = "Predicted", ylab = "Actual",col="blue")
grid()
abline(0, 1, col = "darkorange", lwd = 2)

#density plot
plot(density(test_pred_forest),col="blue",lwd=2,main = "Predited and Actual Value")
lines(density(rf_validation$ENERGYSTARScore),col="green",lwd=2)
legend("topright",legend=c("test value","prediction"),pch=19, col=c("green","blue"))


#feature importance using shapley value
X = rf_validation[which(names(rf_validation) != "ENERGYSTARScore")]
predictorr = Predictor$new(output_forest, data = X, y = rf_validation$ENERGYSTARScore)
shapley = Shapley$new(predictorr, x.interest = X[1,])
shapley$plot()
output_forest



#shapeley value 
dif<-test_pred_forest-rf_validation$ENERGYSTARScore
test_Predicted<-test_pred_forest
oo<-cbind(rf_validation$ENERGYSTARScore,test_Predicted,dif)
oo<-as.data.frame(oo)


which.max(oo$ll)#4020, 1582 on test
subset(rf_validation[1582,])
shapley = Shapley$new(predictorr, x.interest = X[1582,])
plot(shapley)


-----------------------------------------------------------------------------------------------------------
  #knn method

dt_knn = sort(sample(nrow(nn_data), nrow(nn_data)*.7))
knn_train<-nn_data[dt_knn,]
knn_validation<-nn_data[-dt_knn,]

knn_train_labels <- nn_data[dt_knn, 1]
knn_test_labels <- nn_data[-dt_knn, 1]


knn_test_pred <- knn(train = knn_train, test = knn_validation,cl = knn_train_labels, k=9)
test_pred_knn <- predict(knn_test_pred,knn_validation)

MAE_knn<-mean(abs(test_pred_knn-knn_validation$ENERGYSTARScore))
RMSE_knn<-sqrt(mean((test_pred_knn-knn_validation$ENERGYSTARScore)^2))

#https://rpubs.com/mohitagr18/273075
control <- trainControl(method='repeatedcv', number=10, repeats=3)
metric <- 'RMSE'
set.seed(101)
fit_knn <- train(ENERGYSTARScore~., data=knn_train, method='knn', metric=metric, 
            preProc=c('center', 'scale'), trControl=control)
fit_knn <- train(ENERGYSTARScore~., data=nn_data, method='knn', metric=metric, 
                 preProc=c('center', 'scale'), trControl=control)

fit_knn

knn_cv<-knn.cv(knn_train, knn_train_labels, k = 1, l = 0, prob = FALSE, use.all = TRUE)

------------------------------------------------------------------------------------------------------------
#xgboost

  
dtrf = sort(sample(nrow(nn_data), nrow(nn_data)*.7))
xg_train<-nn_data[dtrf,]
xg_validation<-nn_data[-dtrf,]

dtrain <- xgb.DMatrix(data = as.matrix(xg_train[!names(xg_train) %in% c("ENERGYSTARScore")]), label = xg_train$ENERGYSTARScore)
energy_xgb = xgboost(data=dtrain, max_depth=6, eta = 0.2, nthread=3, nrounds=40, lambda=0
                     , objective="reg:linear")

dtest <- as.matrix(xg_validation[!names(xg_train) %in% c("ENERGYSTARScore")])
yhat_xgb <- predict(energy_xgb,dtest)
round(mean((yhat_xgb - xg_validation)^2),2)  

RMSE_xgboost<-sqrt(mean((yhat_xgb-xg_validation$ENERGYSTARScore)^2))
#0.1515
MAE_xgboost <- mean(abs(yhat_xgb-xg_validation$ENERGYSTARScore))
#0.1167


# below code are from https://rpubs.com/superseer/decision_trees_for_regression
set.seed(1)
param <- list("max_depth" = 3, "eta" = 0.2, "objective" = "reg:linear", "lambda" = 0)
cv_nround <- 500
cv_nfold <- 3
energy_xgb_cv <- xgb.cv(param=param, data = dtrain, nfold = cv_nfold, nrounds=cv_nround,
                        early_stopping_rounds = 200, # training will stop if performance doesn't improve for 200 rounds from the last best iteration
                        verbose=0)
#best iteration is 20
dtrain <- xgb.DMatrix(data = as.matrix(xg_train[!names(xg_train) %in% c("ENERGYSTARScore")]), label = xg_train$ENERGYSTARScore)
energy_xgb = xgboost(param=param, data=dtrain, nthread=3, nrounds=energy_xgb_cv$best_iteration, verbose=0)
dtest <- as.matrix(xg_validation[!names(xg_train) %in% c("ENERGYSTARScore")])
yhat_xgb <- predict(energy_xgb,dtest)
round(mean((yhat_xgb - xg_validation)^2),2)

ntrees <- energy_xgb_cv$best_iteration
param_grid <- expand.grid(
  nrounds = ntrees,
  eta = seq(2,24,2)/ntrees,
  #eta = c(0.1, 0.2, 0.3, 0.4, 0.5),
  subsample = 1.0,
  colsample_bytree = 1.0,
  max_depth = c(1,2,3,4,5,6),
  gamma = 1,
  min_child_weight = 1
)

xgb_control <- trainControl(
  method="cv",
  number = 5
)
set.seed(1)
energy_xgb_tuned <- train(ENERGYSTARScore~., data=xg_train, trControl=xgb_control, tuneGrid=param_grid,lambda=0, method="xgbTree")





























